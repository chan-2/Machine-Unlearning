# Machine_Unlearning


<img src="https://github.com/DASH-Lab/Machine_Unlearning/assets/44949723/47181cf5-c718-4f28-898b-949078e8d800.png" width="600" height="300"/>


This paper presents a selective unlearning technique for deep learning models to protect personal information. This technique involves forgetting some of the previously learned knowledge. Existing methods, such as data modification or model retraining, are computationally expensive and often result in performance degradation. As an alternative, our work proposes a gradient-ascent based approach that selectively forgets a specific class, while preserving knowledge learned from different classes. Our method achieves selective unlearning performance with 9 times fewer computational cost, compared to the existing method that trains from scratch.
